
@misc{zhang_ernie_2019,
	title = {{ERNIE}: Enhanced Language Representation with Informative Entities},
	url = {http://arxiv.org/abs/1905.07129},
	shorttitle = {{ERNIE}},
	abstract = {Neural language representation models such as {BERT} pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various {NLP} tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs ({KGs}), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in {KGs} can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and {KGs} to train an enhanced language representation model ({ERNIE}), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that {ERNIE} achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model {BERT} on other common {NLP} tasks. The source code of this paper can be obtained from https://github.com/thunlp/{ERNIE}.},
	number = {{arXiv}:1905.07129},
	publisher = {{arXiv}},
	author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
	urldate = {2022-11-02},
	date = {2019-06-04},
	eprinttype = {arxiv},
	eprint = {1905.07129 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Users\\sebastian.castillo\\Zotero\\storage\\H6QNR4Y5\\Zhang et al. - 2019 - ERNIE Enhanced Language Representation with Infor.pdf:application/pdf;arXiv.org Snapshot:D\:\\Users\\sebastian.castillo\\Zotero\\storage\\RB76GE3C\\1905.html:text/html},
}

@inproceedings{leitner_fine-grained_2019,
	location = {Cham},
	title = {Fine-Grained Named Entity Recognition in Legal Documents},
	isbn = {978-3-030-33220-4},
	doi = {10.1007/978-3-030-33220-4_20},
	series = {Lecture Notes in Computer Science},
	abstract = {This paper describes an approach at Named Entity Recognition ({NER}) in German language documents from the legal domain. For this purpose, a dataset consisting of German court decisions was developed. The source texts were manually annotated with 19 semantic classes: person, judge, lawyer, country, city, street, landscape, organization, company, institution, court, brand, law, ordinance, European legal norm, regulation, contract, court decision, and legal literature. The dataset consists of approx. 67,000 sentences and contains 54,000 annotated entities. The 19 fine-grained classes were automatically generalised to seven more coarse-grained classes (person, location, organization, legal norm, case-by-case regulation, court decision, and legal literature). Thus, the dataset includes two annotation variants, i.e., coarse- and fine-grained. For the task of {NER}, Conditional Random Fields ({CRFs}) and bidirectional Long-Short Term Memory Networks ({BiLSTMs}) were applied to the dataset as state of the art models. Three different models were developed for each of these two model families and tested with the coarse- and fine-grained annotations. The {BiLSTM} models achieve the best performance with an 95.46 F\$\$\_1\$\$score for the fine-grained classes and 95.95 for the coarse-grained ones. The {CRF} models reach a maximum of 93.23 for the fine-grained classes and 93.22 for the coarse-grained ones. The work presented in this paper was carried out under the umbrella of the European project {LYNX} that develops a semantic platform that enables the development of various document processing and analysis applications for the legal domain.},
	pages = {272--287},
	booktitle = {Semantic Systems. The Power of {AI} and Knowledge Graphs},
	publisher = {Springer International Publishing},
	author = {Leitner, Elena and Rehm, Georg and Moreno-Schneider, Julian},
	editor = {Acosta, Maribel and Cudré-Mauroux, Philippe and Maleshkova, Maria and Pellegrini, Tassilo and Sack, Harald and Sure-Vetter, York},
	date = {2019},
	langid = {english},
	keywords = {{BiLSTM}, {CRF}, Curation technologies, Language technology, Legal processing, Legal technologies, {LT}, Named Entity Recognition, Natural Language Processing, {NER}, {NLP}},
	file = {Full Text PDF:D\:\\Users\\sebastian.castillo\\Zotero\\storage\\QD7EG7UY\\Leitner et al. - 2019 - Fine-Grained Named Entity Recognition in Legal Doc.pdf:application/pdf},
}

@online{noauthor_annotated_nodate,
	title = {The Annotated Transformer},
	url = {http://nlp.seas.harvard.edu/annotated-transformer/},
	urldate = {2022-11-02},
	file = {The Annotated Transformer:D\:\\Users\\sebastian.castillo\\Zotero\\storage\\AH9CHVLF\\annotated-transformer.html:text/html},
}

@misc{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2022-11-02},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:D\:\\Users\\sebastian.castillo\\Zotero\\storage\\ELQ2A888\\Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:D\:\\Users\\sebastian.castillo\\Zotero\\storage\\U6DY9ABA\\1706.html:text/html},
}

@inproceedings{yu_named_2020,
	location = {Online},
	title = {Named Entity Recognition as Dependency Parsing},
	url = {https://aclanthology.org/2020.acl-main.577},
	doi = {10.18653/v1/2020.acl-main.577},
	abstract = {Named Entity Recognition ({NER}) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities. {NER} research is often focused on flat entities only (flat {NER}), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009). In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately. We show that the model works well for both nested and flat {NER} through evaluation on 8 corpora and achieving {SoTA} performance on all of them, with accuracy gains of up to 2.2 percentage points.},
	eventtitle = {{ACL} 2020},
	pages = {6470--6476},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Yu, Juntao and Bohnet, Bernd and Poesio, Massimo},
	urldate = {2022-11-03},
	date = {2020-07},
	file = {Full Text PDF:D\:\\Users\\sebastian.castillo\\Zotero\\storage\\EYVAXWDS\\Yu et al. - 2020 - Named Entity Recognition as Dependency Parsing.pdf:application/pdf},
}

@inproceedings{akbik_pooled_2019,
	location = {Minneapolis, Minnesota},
	title = {Pooled Contextualized Embeddings for Named Entity Recognition},
	url = {https://aclanthology.org/N19-1078},
	doi = {10.18653/v1/N19-1078},
	abstract = {Contextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks. They are based on character-level language models which treat text as distributions over characters and are capable of generating embeddings for any string of characters within any textual context. However, such purely character-based approaches struggle to produce meaningful embeddings if a rare string is used in a underspecified context. To address this drawback, we propose a method in which we dynamically aggregate contextualized embeddings of each unique string that we encounter. We then use a pooling operation to distill a ”global” word representation from all contextualized instances. We evaluate these ”pooled contextualized embeddings” on common named entity recognition ({NER}) tasks such as {CoNLL}-03 and {WNUT} and show that our approach significantly improves the state-of-the-art for {NER}. We make all code and pre-trained models available to the research community for use and reproduction.},
	eventtitle = {{NAACL}-{HLT} 2019},
	pages = {724--728},
	booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Akbik, Alan and Bergmann, Tanja and Vollgraf, Roland},
	urldate = {2022-11-03},
	date = {2019-06},
	file = {Full Text PDF:D\:\\Users\\sebastian.castillo\\Zotero\\storage\\2A9SIPQ2\\Akbik et al. - 2019 - Pooled Contextualized Embeddings for Named Entity .pdf:application/pdf},
}

@article{rogers_primer_2020,
	title = {A Primer in {BERTology}: What We Know About How {BERT} Works},
	volume = {8},
	issn = {2307-387X},
	url = {https://direct.mit.edu/tacl/article/96482},
	doi = {10.1162/tacl_a_00349},
	shorttitle = {A Primer in {BERTology}},
	abstract = {Transformer-based models have pushed state of the art in many areas of {NLP}, but our understanding of what is behind their success is still limited. This paper is the ﬁrst survey of over 150 studies of the popular {BERT} model. We review the current state of knowledge about how {BERT} works, what kind of information it learns and how it is represented, common modiﬁcations to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
	pages = {842--866},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	shortjournal = {Transactions of the Association for Computational Linguistics},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	urldate = {2022-11-03},
	date = {2020-12},
	langid = {english},
	file = {Rogers et al. - 2020 - A Primer in BERTology What We Know About How BERT.pdf:D\:\\Users\\sebastian.castillo\\Zotero\\storage\\MII4XP9S\\Rogers et al. - 2020 - A Primer in BERTology What We Know About How BERT.pdf:application/pdf},
}

@online{href_how_nodate,
	title = {How does in-context learning work? A framework for understanding the differences from traditional supervised learning},
	url = {http://ai.stanford.edu/blog/understanding-incontext/},
	shorttitle = {How does in-context learning work?},
	abstract = {The official Stanford {AI} Lab blog},
	author = {href=, \{{\textbackslash}textless\}a},
	urldate = {2022-11-03},
	file = {Snapshot:D\:\\Users\\sebastian.castillo\\Zotero\\storage\\VY8R7JNJ\\understanding-incontext.html:text/html},
}

@inproceedings{cardellino_low-cost_2017,
	location = {Londres, United Kingdom},
	title = {A Low-cost, High-coverage Legal Named Entity Recognizer, Classifier and Linker},
	url = {https://hal.archives-ouvertes.fr/hal-01541446},
	abstract = {In this paper, we try to improve Information Extraction in legal texts by creating a legal Named Entity Recognizer, Classifier and Linker. With this tool, we can identify relevant parts of texts and connect them to a structured knowledge representation, the {LKIF} ontology.
More interestingly, this tool has been developed with relatively little effort, by mapping the {LKIF} ontology to the {YAGO} ontology and through it, taking advantage of the mentions of entities in the Wikipedia. These mentions are used as manually annotated examples to train the Named Entity Recognizer, Classifier and Linker.
We have evaluated the approach on holdout texts from the Wikipedia and also on a small sample of judgments of the European Court of Human Rights, resulting in a very good performance, i.e., around 80\% F-measure for different levels of granularity. We present an extensive error analysis to direct further developments, and we expect that this approach can be successfully ported to other legal subdomains, represented by different ontologies.},
	pages = {22},
	booktitle = {{ICAIL}-2017 - 16th International Conference on Artificial Intelligence and Law},
	author = {Cardellino, Cristian and Teruel, Milagro and Alonso Alemany, Laura and Villata, Serena},
	urldate = {2022-11-03},
	date = {2017-06},
	keywords = {Legal information extraction, legal ontologies, {NER}},
	file = {HAL PDF Full Text:D\:\\Users\\sebastian.castillo\\Zotero\\storage\\I8UJZXLX\\Cardellino et al. - 2017 - A Low-cost, High-coverage Legal Named Entity Recog.pdf:application/pdf},
}

@article{bustos_entrenamiento_nodate,
	title = {{ENTRENAMIENTO} Y {EVALUACIÓN} {DE} {MODELOS} {PEQUEÑOS} {DE} {LENGUAJE} {NATURAL} {BASADO} {EN} {MÉTODOS} {DE} {AUTOATENCIÓN}},
	pages = {41},
	author = {Bustos, Sebastián Alejandro Donoso and Rojas, Jorge Pérez and Mendoza, Iván Sipiran and Gerosa, Daniel Perovich},
	langid = {spanish},
	file = {Bustos et al. - ENTRENAMIENTO Y EVALUACIÓN DE MODELOS PEQUEÑOS DE .pdf:D\:\\Users\\sebastian.castillo\\Zotero\\storage\\B5ND88VL\\Bustos et al. - ENTRENAMIENTO Y EVALUACIÓN DE MODELOS PEQUEÑOS DE .pdf:application/pdf},
}

@misc{sanh_distilbert_2020,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	url = {http://arxiv.org/abs/1910.01108},
	doi = {10.48550/arXiv.1910.01108},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing ({NLP}), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called {DistilBERT}, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a {BERT} model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	number = {{arXiv}:1910.01108},
	publisher = {{arXiv}},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	urldate = {2022-11-03},
	date = {2020-02-29},
	eprinttype = {arxiv},
	eprint = {1910.01108 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:D\:\\Users\\sebastian.castillo\\Zotero\\storage\\73X6T6JP\\Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf:application/pdf;arXiv.org Snapshot:D\:\\Users\\sebastian.castillo\\Zotero\\storage\\KTIIKN5P\\1910.html:text/html},
}

@article{gardos_maintenance_1976,
	title = {Maintenance antipsychotic therapy: is the cure worse than the disease?},
	volume = {133},
	issn = {0002-953X},
	doi = {10.1176/ajp.133.1.32},
	shorttitle = {Maintenance antipsychotic therapy},
	abstract = {The serious long-term complications of maintenance antipsychotic therapy led the authors to undertake a critical review of outpatient withdrawal studies. Key findings included the following: 1) for a least 40\% of outpatient schizophrenics, drugs seem to be essential for survival in the community; 2) the majority of patients who relapse after drug withdrawal recompensate fairly rapidly upon reinstitution of antipsychotic drug therapy; 3) placebo survivors seem to function as well as drug survivors--thus the benefit of maintenance drug therapy appears to be prevention of relapse; and 4) some cases of early relapse after drug withdrawal may be due to dyskinesia rather than psychotic decompensation. The authors urge clinicians to evaluate each patient on maintenance antipsychotic therapy in terms of feasibility of drug withdrawal and offer practical guidelines for withdrawal and subsequent management.},
	pages = {32--36},
	number = {1},
	journaltitle = {The American Journal of Psychiatry},
	shortjournal = {Am J Psychiatry},
	author = {Gardos, G. and Cole, J. O.},
	date = {1976-01},
	pmid = {2021},
	keywords = {Antipsychotic Agents, Basal Ganglia Diseases, Chlorpromazine, Chronic Disease, Clinical Trials as Topic, Female, Humans, Movement Disorders, Promazine, Recurrence, Schizophrenia, Trifluoperazine},
	file = {11.pdf:D\:\\Users\\sebastian.castillo\\Zotero\\storage\\8RX54UPV\\11.pdf:application/pdf},
}

@misc{vaswani_attention_2017-1,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2022-11-03},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:D\:\\Users\\sebastian.castillo\\Zotero\\storage\\6KBERQZQ\\Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:D\:\\Users\\sebastian.castillo\\Zotero\\storage\\VLBTXJZK\\1706.html:text/html},
}

@article{vaswani2017,
	title = {Attention Is All You Need},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
	month = {12},
	date = {2017-12-05},
	doi = {10.48550/arXiv.1706.03762},
	url = {http://arxiv.org/abs/1706.03762},
	note = {arXiv:1706.03762 [cs]}
}

@misc{horacio,
	title = {Horacio Rosatti: {\textquotedblleft}Las sentencias judiciales deben ser profundas y claras{\textquotedblright} {\textendash} Poder Judicial de Entre Ríos},
	url = {https://www.jusentrerios.gov.ar/2022/10/27/horacio-rosatti-las-sentencias-judiciales-deben-ser-profundas-y-claras/},
	langid = {es}
}

@misc{rosatti2022,
	title = {Las sentencias judiciales deben ser profundas y claras},
	author = {Rosatti, Horacio},
	year = {2022},
	month = {10},
	date = {2022-10-27},
	url = {https://www.jusentrerios.gov.ar/2022/10/27/horacio-rosatti-las-sentencias-judiciales-deben-ser-profundas-y-claras/},
	langid = {es}
}

@article{samy2021,
	title = {Reconocimiento y clasificación de entidades nombradas en textos legales en español},
	author = {Samy, Doaa},
	year = {2021},
	date = {2021},
	journal = {Procesamiento del Lenguaje Natural},
	pages = {103--114},
	doi = {10.26342/2021-67-9},
	url = {https://doi.org/10.26342/2021-67-9},
	langid = {es}
}

@article{gutiérrez-fandiño2021,
	title = {Spanish Legalese Language Model and Corpora},
	author = {{Gutiérrez-Fandiño}, Asier and {Armengol-Estapé}, Jordi and Gonzalez-Agirre, Aitor and Villegas, Marta},
	year = {2021},
	month = {10},
	date = {2021-10-23},
	url = {http://arxiv.org/abs/2110.12201},
	note = {arXiv:2110.12201 [cs]}
}

@misc{gobiernoespaña,
	title = {Plan de Tecnologías del Lenguaje - Página principal del Plan de Impulso de las tecnologías del Lenguaje},
	author = {{Gobierno España}, },
	url = {https://plantl.mineco.gob.es/Paginas/index.aspx}
}

@article{serrano2022,
	title = {RigoBERTa: A State-of-the-Art Language Model For Spanish},
	author = {Serrano, Alejandro Vaca and Subies, Guillem Garcia and Zamorano, Helena Montoro and Garcia, Nuria Aldama and Samy, Doaa and Sanchez, David Betancur and Sandoval, Antonio Moreno and Nieto, Marta Guerrero and Jimenez, Alvaro Barbero},
	year = {2022},
	month = {06},
	date = {2022-06-03},
	url = {http://arxiv.org/abs/2205.10233},
	note = {arXiv:2205.10233 [cs]}
}

@article{roy2021,
	title = {Recent Trends in Named Entity Recognition (NER)},
	author = {Roy, Arya},
	year = {2021},
	month = {01},
	date = {2021-01-25},
	url = {http://arxiv.org/abs/2101.11420},
	note = {arXiv:2101.11420 [cs]}
}

@book{jurafsky2021,
	title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
	author = {Jurafsky, Daniel and Martin, James H.},
	year = {2021},
	month = {12},
	date = {2021-12-29},
	edition = {2021}
}
